{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda will be used\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(str(device) + ' will be used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __number_sampler(max_digit=3):\n",
    "    digits = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    k = random.randint(1,max_digit)\n",
    "    string = ''.join(random.sample(digits,k))\n",
    "    if string[0] == '0':\n",
    "        return __number_sampler(max_digit=max_digit)\n",
    "    return string\n",
    "\n",
    "def __operator_sampler():\n",
    "    operators = ['+','-','/','*']\n",
    "    return random.choice(operators)\n",
    "\n",
    "def get_random_input(n_input=100, max_len_limit=15, max_digit=3):\n",
    "    ret = []\n",
    "    for i in range(n_input):\n",
    "        length = random.randint(1,max_len_limit)\n",
    "        length = length if length%2==1 else length+1\n",
    "        #length = max_len_limit # added for simplicity, later delete this line to make problem more complicated\n",
    "        s = ''\n",
    "        for j in range(length):\n",
    "            if j % 2 == 0:\n",
    "                s += __number_sampler(max_digit=max_digit)\n",
    "            else:\n",
    "                s += __operator_sampler()\n",
    "        while len(s) < max_len_limit: s += ' ' ## for variant input\n",
    "        ret.append(s)\n",
    "    return ret\n",
    "\n",
    "def __one_hot_encoding(alphabet, max_len):\n",
    "    x = np.zeros((len(alphabet),len(alphabet)),dtype=np.int32)\n",
    "    dict_ = {}\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i,i] = 1\n",
    "        dict_[alphabet[i]] = x[i]\n",
    "    return dict_\n",
    "\n",
    "def create_input_seq(alphabet, max_len, input_sequence):\n",
    "    alphabet2encod = __one_hot_encoding(alphabet, max_len)\n",
    "    ret = np.zeros((len(input_sequence),max_len,len(alphabet)))\n",
    "    for j, inp in enumerate(input_sequence):\n",
    "        temp = np.zeros((len(inp),len(alphabet)))\n",
    "        for i in range(temp.shape[0]):\n",
    "            temp[i,:] = alphabet2encod[inp[i]]\n",
    "        ret[j,:,:] = temp\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_feature_num, hidden_feature_num=200, n_layers=1, out_dim=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.input_feature_num = input_feature_num\n",
    "        self.hidden_feature_num = hidden_feature_num\n",
    "        self.n_layers = n_layers\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.rnn = nn.RNN(input_feature_num, hidden_feature_num, n_layers)\n",
    "        self.fc = nn.Linear(hidden_feature_num, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(1)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        hidden = hidden.to(device)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        hidden = hidden.contiguous().view(-1, self.hidden_feature_num)\n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_feature_num)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 7 # must be an odd number\n",
    "n_input = 10000\n",
    "alphabet = ['0','1','2','3','4','5','6','7','8','9','-','+','/','*',' '] \n",
    "target_values = []\n",
    "input_feature_num = len(alphabet)\n",
    "\n",
    "input_sequence = get_random_input(n_input=n_input, max_len_limit=max_len, max_digit=1)\n",
    "one_hot_input = create_input_seq(alphabet, max_len, input_sequence)\n",
    "input_seq = torch.FloatTensor(one_hot_input).permute(1,0,2)\n",
    "\n",
    "for i in range(len(input_sequence)):\n",
    "    target_values.append(eval(input_sequence[i]))\n",
    "\n",
    "target_values = torch.FloatTensor(target_values).unsqueeze(1)\n",
    "target_values = target_values.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (rnn): RNN(15, 200)\n",
       "  (fc): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(input_feature_num=input_feature_num)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "lr=0.01\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10000............. Loss: 7918.0400\n",
      "Epoch: 20/10000............. Loss: 7849.9585\n",
      "Epoch: 30/10000............. Loss: 7819.2495\n",
      "Epoch: 40/10000............. Loss: 7837.9014\n",
      "Epoch: 50/10000............. Loss: 7883.2593\n",
      "Epoch: 60/10000............. Loss: 7876.6846\n",
      "Epoch: 70/10000............. Loss: 7876.5054\n",
      "Epoch: 80/10000............. Loss: 7875.5327\n",
      "Epoch: 90/10000............. Loss: 7874.1953\n",
      "Epoch: 100/10000............. Loss: 7863.9058\n",
      "Epoch: 110/10000............. Loss: 7838.8032\n",
      "Epoch: 120/10000............. Loss: 7842.2871\n",
      "Epoch: 130/10000............. Loss: 7774.8462\n",
      "Epoch: 140/10000............. Loss: 7712.2344\n",
      "Epoch: 150/10000............. Loss: 7635.6768\n",
      "Epoch: 160/10000............. Loss: 7545.8760\n",
      "Epoch: 170/10000............. Loss: 7566.1729\n",
      "Epoch: 180/10000............. Loss: 7226.6768\n",
      "Epoch: 190/10000............. Loss: 6975.2959\n",
      "Epoch: 200/10000............. Loss: 6712.2231\n",
      "Epoch: 210/10000............. Loss: 6863.0913\n",
      "Epoch: 220/10000............. Loss: 6267.5518\n",
      "Epoch: 230/10000............. Loss: 6035.9155\n",
      "Epoch: 240/10000............. Loss: 5833.9634\n",
      "Epoch: 250/10000............. Loss: 5710.2114\n",
      "Epoch: 260/10000............. Loss: 5557.7266\n",
      "Epoch: 270/10000............. Loss: 5375.2485\n",
      "Epoch: 280/10000............. Loss: 5223.2886\n",
      "Epoch: 290/10000............. Loss: 5068.8320\n",
      "Epoch: 300/10000............. Loss: 4930.2949\n",
      "Epoch: 310/10000............. Loss: 4791.0967\n",
      "Epoch: 320/10000............. Loss: 4664.1855\n",
      "Epoch: 330/10000............. Loss: 4640.1904\n",
      "Epoch: 340/10000............. Loss: 4460.1816\n",
      "Epoch: 350/10000............. Loss: 4408.7559\n",
      "Epoch: 360/10000............. Loss: 4260.5933\n",
      "Epoch: 370/10000............. Loss: 4179.6230\n",
      "Epoch: 380/10000............. Loss: 4327.0854\n",
      "Epoch: 390/10000............. Loss: 4155.4360\n",
      "Epoch: 400/10000............. Loss: 4056.7888\n",
      "Epoch: 410/10000............. Loss: 3993.4392\n",
      "Epoch: 420/10000............. Loss: 3898.2920\n",
      "Epoch: 430/10000............. Loss: 3811.3535\n",
      "Epoch: 440/10000............. Loss: 3733.3884\n",
      "Epoch: 450/10000............. Loss: 3674.1265\n",
      "Epoch: 460/10000............. Loss: 3775.3372\n",
      "Epoch: 470/10000............. Loss: 3710.3081\n",
      "Epoch: 480/10000............. Loss: 3558.6824\n",
      "Epoch: 490/10000............. Loss: 3445.0383\n",
      "Epoch: 500/10000............. Loss: 3371.7720\n",
      "Epoch: 510/10000............. Loss: 3307.3015\n",
      "Epoch: 520/10000............. Loss: 3264.0747\n",
      "Epoch: 530/10000............. Loss: 3202.7192\n",
      "Epoch: 540/10000............. Loss: 3132.5344\n",
      "Epoch: 550/10000............. Loss: 3091.0889\n",
      "Epoch: 560/10000............. Loss: 3027.4143\n",
      "Epoch: 570/10000............. Loss: 2976.1738\n",
      "Epoch: 580/10000............. Loss: 2927.4905\n",
      "Epoch: 590/10000............. Loss: 2905.6721\n",
      "Epoch: 600/10000............. Loss: 2844.4299\n",
      "Epoch: 610/10000............. Loss: 2812.2727\n",
      "Epoch: 620/10000............. Loss: 2745.5449\n",
      "Epoch: 630/10000............. Loss: 2726.7480\n",
      "Epoch: 640/10000............. Loss: 2673.8843\n",
      "Epoch: 650/10000............. Loss: 2618.7927\n",
      "Epoch: 660/10000............. Loss: 2580.0320\n",
      "Epoch: 670/10000............. Loss: 2557.5657\n",
      "Epoch: 680/10000............. Loss: 2496.2163\n",
      "Epoch: 690/10000............. Loss: 2453.3108\n",
      "Epoch: 700/10000............. Loss: 2415.7068\n",
      "Epoch: 710/10000............. Loss: 2381.5420\n",
      "Epoch: 720/10000............. Loss: 2346.2351\n",
      "Epoch: 730/10000............. Loss: 2331.6245\n",
      "Epoch: 740/10000............. Loss: 2293.3428\n",
      "Epoch: 750/10000............. Loss: 2251.4668\n",
      "Epoch: 760/10000............. Loss: 2215.7495\n",
      "Epoch: 770/10000............. Loss: 2183.7988\n",
      "Epoch: 780/10000............. Loss: 2163.1768\n",
      "Epoch: 790/10000............. Loss: 2123.8745\n",
      "Epoch: 800/10000............. Loss: 2093.6641\n",
      "Epoch: 810/10000............. Loss: 2064.8843\n",
      "Epoch: 820/10000............. Loss: 2044.5576\n",
      "Epoch: 830/10000............. Loss: 2011.6056\n",
      "Epoch: 840/10000............. Loss: 1981.3575\n",
      "Epoch: 850/10000............. Loss: 1952.9832\n",
      "Epoch: 860/10000............. Loss: 1937.9764\n",
      "Epoch: 870/10000............. Loss: 1901.9800\n",
      "Epoch: 880/10000............. Loss: 1876.9448\n",
      "Epoch: 890/10000............. Loss: 1850.4432\n",
      "Epoch: 900/10000............. Loss: 1828.7792\n",
      "Epoch: 910/10000............. Loss: 1950.8503\n",
      "Epoch: 920/10000............. Loss: 24428.8691\n",
      "Epoch: 930/10000............. Loss: 8025.5986\n",
      "Epoch: 940/10000............. Loss: 8104.1792\n",
      "Epoch: 950/10000............. Loss: 7913.9424\n",
      "Epoch: 960/10000............. Loss: 7937.1040\n",
      "Epoch: 970/10000............. Loss: 7913.9722\n",
      "Epoch: 980/10000............. Loss: 7916.7183\n",
      "Epoch: 990/10000............. Loss: 7913.9272\n",
      "Epoch: 1000/10000............. Loss: 7914.1982\n",
      "Epoch: 1010/10000............. Loss: 7913.7617\n",
      "Epoch: 1020/10000............. Loss: 7912.1025\n",
      "Epoch: 1030/10000............. Loss: 7903.2622\n",
      "Epoch: 1040/10000............. Loss: 7890.9238\n",
      "Epoch: 1050/10000............. Loss: 7881.2222\n",
      "Epoch: 1060/10000............. Loss: 7873.6289\n",
      "Epoch: 1070/10000............. Loss: 7867.7046\n",
      "Epoch: 1080/10000............. Loss: 7862.9023\n",
      "Epoch: 1090/10000............. Loss: 7857.9790\n",
      "Epoch: 1100/10000............. Loss: 7851.5840\n",
      "Epoch: 1110/10000............. Loss: 7841.3633\n",
      "Epoch: 1120/10000............. Loss: 7823.6001\n",
      "Epoch: 1130/10000............. Loss: 7800.2217\n",
      "Epoch: 1140/10000............. Loss: 7778.0078\n",
      "Epoch: 1150/10000............. Loss: 7767.1440\n",
      "Epoch: 1160/10000............. Loss: 7758.3560\n",
      "Epoch: 1170/10000............. Loss: 7748.8174\n",
      "Epoch: 1180/10000............. Loss: 7733.8618\n",
      "Epoch: 1190/10000............. Loss: 7728.3394\n",
      "Epoch: 1200/10000............. Loss: 7707.6929\n",
      "Epoch: 1210/10000............. Loss: 7696.0566\n",
      "Epoch: 1220/10000............. Loss: 7882.8975\n",
      "Epoch: 1230/10000............. Loss: 7866.7217\n",
      "Epoch: 1240/10000............. Loss: 7770.6826\n",
      "Epoch: 1250/10000............. Loss: 7707.4658\n",
      "Epoch: 1260/10000............. Loss: 7716.2798\n",
      "Epoch: 1270/10000............. Loss: 7687.6768\n",
      "Epoch: 1280/10000............. Loss: 7686.9214\n",
      "Epoch: 1290/10000............. Loss: 7672.9009\n",
      "Epoch: 1300/10000............. Loss: 7670.0142\n",
      "Epoch: 1310/10000............. Loss: 7671.9482\n",
      "Epoch: 1320/10000............. Loss: 7745.6479\n",
      "Epoch: 1330/10000............. Loss: 7670.5986\n",
      "Epoch: 1340/10000............. Loss: 7651.4302\n",
      "Epoch: 1350/10000............. Loss: 7651.0254\n",
      "Epoch: 1360/10000............. Loss: 7642.5854\n",
      "Epoch: 1370/10000............. Loss: 7620.9058\n",
      "Epoch: 1380/10000............. Loss: 7613.1040\n",
      "Epoch: 1390/10000............. Loss: 7630.0410\n",
      "Epoch: 1400/10000............. Loss: 7621.8838\n",
      "Epoch: 1410/10000............. Loss: 7597.7490\n",
      "Epoch: 1420/10000............. Loss: 7606.3662\n",
      "Epoch: 1430/10000............. Loss: 7617.1152\n",
      "Epoch: 1440/10000............. Loss: 7634.8330\n",
      "Epoch: 1450/10000............. Loss: 7594.2217\n",
      "Epoch: 1460/10000............. Loss: 7573.9263\n",
      "Epoch: 1470/10000............. Loss: 7572.7407\n",
      "Epoch: 1480/10000............. Loss: 7563.8130\n",
      "Epoch: 1490/10000............. Loss: 7562.0498\n",
      "Epoch: 1500/10000............. Loss: 7593.3887\n",
      "Epoch: 1510/10000............. Loss: 7648.6694\n",
      "Epoch: 1520/10000............. Loss: 7555.1968\n",
      "Epoch: 1530/10000............. Loss: 7551.4194\n",
      "Epoch: 1540/10000............. Loss: 7543.7778\n",
      "Epoch: 1550/10000............. Loss: 7534.2031\n",
      "Epoch: 1560/10000............. Loss: 7528.3193\n",
      "Epoch: 1570/10000............. Loss: 7521.0791\n",
      "Epoch: 1580/10000............. Loss: 7558.5615\n",
      "Epoch: 1590/10000............. Loss: 7423.6138\n",
      "Epoch: 1600/10000............. Loss: 7911.0640\n",
      "Epoch: 1610/10000............. Loss: 7915.3545\n",
      "Epoch: 1620/10000............. Loss: 7915.2432\n",
      "Epoch: 1630/10000............. Loss: 7913.9839\n",
      "Epoch: 1640/10000............. Loss: 7914.0488\n",
      "Epoch: 1650/10000............. Loss: 7914.0039\n",
      "Epoch: 1660/10000............. Loss: 7913.9551\n",
      "Epoch: 1670/10000............. Loss: 7913.9600\n",
      "Epoch: 1680/10000............. Loss: 7913.9570\n",
      "Epoch: 1690/10000............. Loss: 7913.9551\n",
      "Epoch: 1700/10000............. Loss: 7913.9546\n",
      "Epoch: 1710/10000............. Loss: 7913.9546\n",
      "Epoch: 1720/10000............. Loss: 7913.9546\n",
      "Epoch: 1730/10000............. Loss: 7913.9551\n",
      "Epoch: 1740/10000............. Loss: 7913.9551\n",
      "Epoch: 1750/10000............. Loss: 7913.9551\n",
      "Epoch: 1760/10000............. "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-84575ab8a592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}/{}.............'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1,n_epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    input_seq = input_seq.to(device)\n",
    "    output = model(input_seq)\n",
    "    loss = loss_fn(output, target_values)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, seq):\n",
    "    one_hot_input = create_input_seq(alphabet, max_len, [seq])\n",
    "    input_seq = torch.FloatTensor(one_hot_input).permute(1,0,2)\n",
    "    input_seq = input_seq.to(device)\n",
    "    output = model(input_seq)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = '5+3*2/1'\n",
    "y = '5-3+5-2'\n",
    "z = '2/3/5/7'\n",
    "r = '1+1+1+1'\n",
    "a = predict(model, x).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.4266]], device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

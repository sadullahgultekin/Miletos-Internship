{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from models import Net12, Net8, Net3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "OUT_FILE = 'model_logs/general_log.txt'\n",
    "\n",
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device='cpu'\n",
    "dtype = torch.float32\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image names that will be used\n",
    "with open(\"line_img_names.txt\",\"r\") as file:\n",
    "    line_file_names = file.read().split('\\n')[:-1]\n",
    "\n",
    "with open(\"hw_img_names.txt\",\"r\") as file:\n",
    "    hw_file_names = file.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_img_path = \"/home/sadullah/line-type-classification/data/processed/line\"\n",
    "hw_img_path = \"/home/sadullah/line-type-classification/data/processed/hwline\"\n",
    "\n",
    "line_data = []\n",
    "hw_data = []\n",
    "\n",
    "for line_file_name in line_file_names:\n",
    "    line_data.append(cv2.imread(os.path.join(line_img_path, line_file_name)))\n",
    "    \n",
    "for hw_file_name in hw_file_names:\n",
    "    hw_data.append(cv2.imread(os.path.join(hw_img_path, hw_file_name)))\n",
    "\n",
    "# label of line is '0'\n",
    "# label of hw is '1'\n",
    "\n",
    "# sizes of validation data for line and handwritten sets\n",
    "line_val_size = len(line_data) // 5\n",
    "hw_val_size = len(hw_data) // 5\n",
    "\n",
    "# sizes of test data for line and handwritten sets\n",
    "line_test_size = len(line_data) // 5\n",
    "hw_test_size = len(hw_data) // 5\n",
    "\n",
    "# sizes of tran data for line and handwritten sets\n",
    "line_train_size = len(line_data) - line_val_size - line_test_size\n",
    "hw_train_size = len(hw_data) - hw_val_size - hw_test_size\n",
    "\n",
    "# labels for line sets, which are zeros\n",
    "train_label = np.zeros(line_train_size)\n",
    "val_label = np.zeros(line_val_size)\n",
    "test_label = np.zeros(line_test_size)\n",
    "\n",
    "# add hw labels to the labels, hw labels are ones\n",
    "train_label = np.concatenate((train_label,np.ones(hw_train_size)))\n",
    "val_label = np.concatenate((val_label,np.ones(hw_val_size)))\n",
    "test_label = np.concatenate((test_label,np.ones(hw_test_size)))\n",
    "\n",
    "# split data into train, validation and test\n",
    "train_data = line_data[ : line_train_size]\n",
    "val_data = line_data[line_train_size : line_train_size + line_val_size]\n",
    "test_data = line_data[line_train_size + line_val_size :]\n",
    "\n",
    "# add hw data\n",
    "train_data.extend(hw_data[ : hw_train_size])\n",
    "val_data.extend(hw_data[hw_train_size : hw_train_size + hw_val_size])\n",
    "test_data.extend(hw_data[hw_train_size + hw_val_size :])\n",
    "\n",
    "# make necessary transposes for training model\n",
    "train_data = np.array(train_data)\n",
    "val_data = np.array(val_data)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "#data_means = np.mean(train_data, axis=(0,1,2))\n",
    "#data_stds = np.std(train_data, axis=(0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AspectRatioSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size \n",
    "        self.n_batches = len(self.data_source.data) // self.batch_size + 1\n",
    "        self.sorted = np.array(sorted(range(len(self.data_source.data)), key=lambda k : self.data_source.data[k].shape[1] / self.data_source.data[k].shape[0]))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.n_batches):\n",
    "            start_idx = np.random.randint(len(self.data_source.data) - self.batch_size)\n",
    "            current_batch = self.sorted[start_idx:start_idx+self.batch_size]\n",
    "            yield current_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "        \n",
    "class AlignCollate(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, batch):      \n",
    "        images, labels = list(zip(*batch))\n",
    "        \n",
    "        images = list(images)\n",
    "        labels = list(labels)\n",
    "                \n",
    "        images = self.resize(images)\n",
    "        images = [T.ToTensor()(img) for img in images]\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "        labels = torch.LongTensor(np.array(labels))\n",
    "        \n",
    "        return images, labels\n",
    "\n",
    "    def resize(self, batch):\n",
    "        mean_width, mean_height = 0, 0\n",
    "        for elem in batch:\n",
    "            mean_height += elem.shape[0]\n",
    "            mean_width += elem.shape[1]\n",
    "        mean_width /= len(batch)\n",
    "        mean_height /= len(batch)\n",
    "        batch = [Image.fromarray(cv2.resize(elem, (round(mean_width), round(mean_height)))) for elem in batch]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HwPrintedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sample = self.data[i]\n",
    "        return sample, self.label[i]\n",
    "    \n",
    "batch_size=15\n",
    "\n",
    "train_dataset = HwPrintedDataset(train_data, train_label)\n",
    "train_collate = AlignCollate()\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=AspectRatioSampler(train_dataset, batch_size), pin_memory=True,\n",
    "                          collate_fn=train_collate)\n",
    "\n",
    "val_dataset = HwPrintedDataset(val_data, val_label)\n",
    "val_collate = AlignCollate()\n",
    "val_loader = DataLoader(val_dataset, batch_sampler=AspectRatioSampler(val_dataset, batch_size), pin_memory=True,\n",
    "                        collate_fn=val_collate)\n",
    "\n",
    "test_dataset = HwPrintedDataset(test_data, test_label)\n",
    "test_collate = AlignCollate()\n",
    "test_loader = DataLoader(test_dataset, batch_sampler=AspectRatioSampler(test_dataset, batch_size), pin_memory=True,\n",
    "                         collate_fn=test_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_accuracy_calculation(num_correct_0, num_correct_1, num_samples_0, num_samples_1):\n",
    "\n",
    "    acc_0 = float(num_correct_0) / (num_samples_0 + 1e-9) * 100\n",
    "    acc_1 = float(num_correct_1) / (num_samples_1 + 1e-9) * 100\n",
    "\n",
    "    num_correct = num_correct_0 + num_correct_1\n",
    "    num_samples = num_samples_0 + num_samples_1\n",
    "    acc = float(num_correct) / num_samples * 100\n",
    "    \n",
    "    print('For line: %.2f' % acc_0)\n",
    "    print('For hw: %.2f' % acc_1)\n",
    "\n",
    "    print('General acc: %.2f' % acc)\n",
    "    with open(OUT_FILE, 'a') as f:\n",
    "        f.write('{:<15.4f}{:<15.4f}{:<15.4f}'.format(acc_0,acc_1,acc))\n",
    "    \n",
    "    return acc_0, acc_1, acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(val_loader, model):\n",
    "    num_correct_0, num_correct_1, num_samples_0, num_samples_1 = 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, sample_batch in enumerate(val_loader):\n",
    "            \n",
    "            x, y = sample_batch\n",
    "            \n",
    "            x, y = x.to(device=device, dtype=dtype), y.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            \n",
    "            num_correct_0 += torch.sum(preds[y == 0] == 0)\n",
    "            num_samples_0 += torch.sum(y == 0)\n",
    "            \n",
    "            num_correct_1 += torch.sum(preds[y == 1] == 1) \n",
    "            num_samples_1 += torch.sum(y == 1)\n",
    "        \n",
    "        print('Checking accuracy on validation set')\n",
    "        acc_0, acc1, acc = helper_accuracy_calculation(num_correct_0.item(), num_correct_1.item(), num_samples_0.item(), num_samples_1.item())\n",
    "        \n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(model, optimizer, train_loader, val_loader, epochs=1):\n",
    "    with open(OUT_FILE, 'a') as f:\n",
    "        f.write('\\n\\n\\nTraining started at {} \\n\\n'.format(str(datetime.now())))\n",
    "        f.write('Model name is {} \\n\\n'.format(model.model_name))\n",
    "        f.write('{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}{:<15}'.format('epoch','iteration', 'train_loss', 'train_hw_acc', 'train_line_acc', 'train_gen_acc', 'val_hw_acc', 'val_line_acc', 'val_gen_acc'))\n",
    "\n",
    "    \n",
    "    num_correct_0, num_correct_1, num_samples_0, num_samples_1 = 0, 0, 0, 0\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    model.train()\n",
    "\n",
    "    bets_model = None\n",
    "    best_acc = 0\n",
    "\n",
    "    for e in range(1,epochs+1):\n",
    "        for t, sample_batch in enumerate(train_loader):\n",
    "            \n",
    "            x, y = sample_batch\n",
    "            \n",
    "            x, y = x.to(device=device, dtype=dtype), y.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            scores = model(x)\n",
    "            \n",
    "            ### calclulation of training accuracy ###\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct_0 += torch.sum(preds[y == 0] == 0)\n",
    "            num_samples_0 += torch.sum(y == 0)\n",
    "\n",
    "            num_correct_1 += torch.sum(preds[y == 1] == 1)\n",
    "            num_samples_1 += torch.sum(y == 1)\n",
    "            ### calclulation of training accuracy ###\n",
    "            \n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if t % print_every == 0:\n",
    "                print('Epoch %s, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
    "                with open(OUT_FILE, 'a') as f:\n",
    "                    f.write('\\n{:<15}{:<15.4f}{:<15.4f}'.format(int(e),t,loss.item()))\n",
    "\n",
    "                print('Checking accuracy on training set')\n",
    "                acc_0, acc1, acc = helper_accuracy_calculation(num_correct_0.item(), num_correct_1.item(), num_samples_0.item(), num_samples_1.item())\n",
    "                \n",
    "                val_acc = check_accuracy(val_loader, model)\n",
    "                if val_acc > best_acc:\n",
    "                    best_model = model\n",
    "                    best_acc = val_acc\n",
    "                    time = datetime.now()\n",
    "                    time = str(time.month) + \"-\" + str(time.day) + \"-\" + str(time.hour) + \"-\" + str(time.minute)\n",
    "                    torch.save(model.state_dict(),'/home/sadullah/line-type-classification/data/scripts/model_logs/{0}_{1}_{2}_{3:.4f}_{4}.pth'.format(model.model_name,e,t,val_acc,time))\n",
    "                print()\n",
    "                \n",
    "    with open(OUT_FILE, 'a') as f:\n",
    "        f.write('\\n\\n\\nTraining ended at {} \\n\\n\\n'.format(str(datetime.now())))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "model = Net8()\n",
    "\n",
    "#model.load_state_dict(torch.load('/home/sadullah/line-type-classification/data/scripts/model_logs/8_layered_3_1300_97.22347629796839.pth'))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_net(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
